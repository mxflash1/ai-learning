
What I learned from my research.

I learned that LLMs (Large Language Models) are a type of machine learning model designed to understand and respond to user queries. They are trained on large datasets and learn patterns in language through that data, this process is called machine learning.

It can do machine learning in 3 different ways, supervised, unsupervised and reinforcement. Supervised machine learning is where it can be fed thousands of photos of cats and no cats, these photos have then been tagged with either “cat” or “no cat”, the model then goes through these photos and learns to find patterns and becomes able to identify where there are cats or not in a photo that has not been previously tagged. 

Unsupervised machine learning refers to where this same database has been provided to the LLM, but the data has not been tagged with cat or no cat, and you can just dump the raw photos of the cats, and it can go and find the patterns in the photos that have cats or not. There are 3 main types of unsupervised machine learning, clustering, dimension reduction and association. Clustering is where it groups similar things together (which is what would’ve been used in our cat example as “cat” and “no cat”), dimension reduction is where you strip cluttered and complicated data to its essence to try and then try and find similarities from that (this is done in situations where you want to save time or have super unorganised complicated data). The last type of unsupervised machine learning is association, this is where it finds rules or patterns where it can see if a person is going through chemo, then they are most likely to experience hair loss, the machine looks for patterns like these and groups them.

Finally, reinforcement data refers to rewarding the model as if you were training a dog, it learns by trial and error but with you telling the model whether it is right or wrong so it can learn from its mistakes.

I also learnt about embeddings and that they are numerical representations of words used so that the LLM can understand natural human language. Every single embedding is stored in a library where instead of embeddings being placed in dictionary order, they are placed near similar embeddings. By storing the information like this it allows the LLM’s to have a better understanding of the human language through being able to easily compare things based on meaning.

There are different types of embeddings which are word, sentence, document, graph, image and audio embeddings. Word embeddings are how the neural networks purely just turn a word into an embedding without considering the context it may have in a sentence or text where “punch” might mean hitting someone in the face or it could mean the drink, the word embedding is not going to know the difference and it will have the same embedding even if there is different context. Next, we have sentence/document embeddings which are where the NN’s translate the whole sentence into a single embedding which can be used in situations where you are comparing sentences, since the vectors given to the sentence are based on meaning the model will be able to identify the contrast in-between the vectors (which are the whole sentences). And how this would work for documents is the same but just on a bigger scale where the NN gives a whole document a single embedding, so it understands the meaning of the document instead of just a sentence. 

There are also contextual embeddings which come from transformers where NN’s can now understand context of a word depending on the sentence around it (this category of NN’s are known as transformers), it does this through “Attention”. The transformer can look at a sentence like “Did you just punch him in the face?” or “Have you tried the new punch flavour?” where older models of NN’s would’ve given “punch” the same vector for each sentence when they obviously have very different meanings. This is where attention comes in, the contextual embedding can look at this sentence and pay “attention” to the different contextual clues that would help the model understand what “punch” means in different scenarios. In this sentence, “Did you just punch him in the face?” the Transformer would pay attention to the context that “punch” is put in and would look at the words like “him” or “face” and these contextual clues can now help this transformer vectorize “punch” to be when you hit someone with your fist and not a drink. 

Another embedding format is graph embedding, since there are no words in graphs it is a completely different way of vectorizing the data shown in the graph. Since graphs are based on relationships, what the model can look at to interpret the data in the graph are the similarities in between different variables. So to vectorize the graph what the neural networks do is give things that are similar to each other (a family of 4 in an extremely extended family tree) similar vectors so that that model can now interpret the data and identify that this family of 4 are a family of 4 and not distant relatives because they have similar embeddings.

Last way of imbedding we are going to talk about today is image imbedding. This is arguably one of the most important and most complicated so I’m going to attempt to explain it. The “brains” that convert the image into vectors can be either a transformer or neural network. How the neural network vectorized is through layers, it finds the edges of the image and feels the texture which gives it a grasp of the canvas. It then recognises patterns like eyes, feet and parts of the image that pop out as having a recognisable pattern. The last layer then puts these patterns it found previously together and then recognises full objects and finally the whole image and gives a vector output that the model can use and compare the vectors to the vectors of the images that the model was trained on. That was how the model uses NN’s to vectorize images, now we will delve into how the model can use transformers to vectorize the images. Basically how the model uses transformers to vectorize images is by splitting up the image into patches, it takes these 3D (RGB) patches and flattens them, this means that every single pixel has a red value, green value and blue value so for a 16x16 pixel patch there will be 768 values. How it gets flattened is by listing all these values in a long list of numbers where all the pixels colours are listed so 256 red values will be listed (if it’s a 16x16 patch) then 256 green values and then 256 blue values. This patch then gets stored with the information of where in the picture it came from (position info). I know I said that this part is done with transformers, but this is just the prep, so this vectorization is being done with NN’s at the start (with the prep for the transformer) and is now going to be fed into the transformer. All of these patches are now fed into the transformer one by one and the transformer uses “attention” once again to understand the context of each patch and uses the similarity of the colour of each pixel to understand the bigger picture of the image, for example it sees that the blue sky patch and another blue sky patch have similar RGB values, it’s going to understand that this part of the image is the blue sky and is going to piece that together.
